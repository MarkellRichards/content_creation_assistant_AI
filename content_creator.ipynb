{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ce932353-1a95-4b24-a9d4-04c807e59e31",
   "metadata": {},
   "source": [
    "## Overview\n",
    "\n",
    "The Content Creation Assistant notebook harnesses cutting-edge artificial intelligence to streamline and enhance digital content creation. It integrates advanced tools and frameworks to offer a comprehensive solution for both written and visual content generation.\n",
    "\n",
    "At its core is the LlamaIndex framework, which efficiently manages complex workflows and integrates various AI models and APIs. The Tavily Search API provides real-time, relevant information to ensure that the content is engaging, informative, and timely.\n",
    "\n",
    "For text generation, the notebook utilizes the OpenAI GPT model, known for producing coherent and contextually accurate text, ideal for blog posts and social media updates. Complementing this, the OpenAI DALL-E 3 model generates unique images based on the themes of the text, enhancing visual appeal and reader engagement.\n",
    "\n",
    "Overall, the Content Creation Assistant empowers users—whether bloggers, marketers, or social media managers—to produce rich, multi-dimensional content effortlessly, saving time and boosting creativity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d160ca71-50fc-48f7-8ac9-ac70360645ee",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting llama-index-core\n",
      "  Downloading llama_index_core-0.11.23-py3-none-any.whl.metadata (2.5 kB)\n",
      "Collecting llama-index-utils-workflow\n",
      "  Downloading llama_index_utils_workflow-0.2.2-py3-none-any.whl.metadata (667 bytes)\n",
      "Collecting asyncio\n",
      "  Downloading asyncio-3.4.3-py3-none-any.whl.metadata (1.7 kB)\n",
      "Collecting llama-index-llms-openai\n",
      "  Downloading llama_index_llms_openai-0.2.16-py3-none-any.whl.metadata (3.3 kB)\n",
      "Collecting tavily-python\n",
      "  Downloading tavily_python-0.5.0-py3-none-any.whl.metadata (11 kB)\n",
      "Collecting openai\n",
      "  Downloading openai-1.54.4-py3-none-any.whl.metadata (24 kB)\n",
      "Requirement already satisfied: PyYAML>=6.0.1 in /opt/conda/lib/python3.12/site-packages (from llama-index-core) (6.0.2)\n",
      "Requirement already satisfied: SQLAlchemy>=1.4.49 in /opt/conda/lib/python3.12/site-packages (from SQLAlchemy[asyncio]>=1.4.49->llama-index-core) (2.0.36)\n",
      "Requirement already satisfied: aiohttp<4.0.0,>=3.8.6 in /opt/conda/lib/python3.12/site-packages (from llama-index-core) (3.10.10)\n",
      "Collecting dataclasses-json (from llama-index-core)\n",
      "  Downloading dataclasses_json-0.6.7-py3-none-any.whl.metadata (25 kB)\n",
      "Collecting deprecated>=1.2.9.3 (from llama-index-core)\n",
      "  Downloading Deprecated-1.2.15-py2.py3-none-any.whl.metadata (5.5 kB)\n",
      "Collecting dirtyjson<2.0.0,>=1.0.8 (from llama-index-core)\n",
      "  Downloading dirtyjson-1.0.8-py3-none-any.whl.metadata (11 kB)\n",
      "Collecting filetype<2.0.0,>=1.2.0 (from llama-index-core)\n",
      "  Downloading filetype-1.2.0-py2.py3-none-any.whl.metadata (6.5 kB)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /opt/conda/lib/python3.12/site-packages (from llama-index-core) (2024.10.0)\n",
      "Requirement already satisfied: httpx in /opt/conda/lib/python3.12/site-packages (from llama-index-core) (0.27.2)\n",
      "Requirement already satisfied: nest-asyncio<2.0.0,>=1.5.8 in /opt/conda/lib/python3.12/site-packages (from llama-index-core) (1.6.0)\n",
      "Requirement already satisfied: networkx>=3.0 in /opt/conda/lib/python3.12/site-packages (from llama-index-core) (3.4.2)\n",
      "Collecting nltk>3.8.1 (from llama-index-core)\n",
      "  Downloading nltk-3.9.1-py3-none-any.whl.metadata (2.9 kB)\n",
      "Collecting numpy<2.0.0 (from llama-index-core)\n",
      "  Downloading numpy-1.26.4-cp312-cp312-manylinux_2_17_aarch64.manylinux2014_aarch64.whl.metadata (62 kB)\n",
      "Requirement already satisfied: pillow>=9.0.0 in /opt/conda/lib/python3.12/site-packages (from llama-index-core) (11.0.0)\n",
      "Requirement already satisfied: pydantic<3.0.0,>=2.7.0 in /opt/conda/lib/python3.12/site-packages (from llama-index-core) (2.9.2)\n",
      "Requirement already satisfied: requests>=2.31.0 in /opt/conda/lib/python3.12/site-packages (from llama-index-core) (2.32.3)\n",
      "Collecting tenacity!=8.4.0,<9.0.0,>=8.2.0 (from llama-index-core)\n",
      "  Downloading tenacity-8.5.0-py3-none-any.whl.metadata (1.2 kB)\n",
      "Collecting tiktoken>=0.3.3 (from llama-index-core)\n",
      "  Downloading tiktoken-0.8.0-cp312-cp312-manylinux_2_17_aarch64.manylinux2014_aarch64.whl.metadata (6.6 kB)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.66.1 in /opt/conda/lib/python3.12/site-packages (from llama-index-core) (4.67.0)\n",
      "Requirement already satisfied: typing-extensions>=4.5.0 in /opt/conda/lib/python3.12/site-packages (from llama-index-core) (4.12.2)\n",
      "Collecting typing-inspect>=0.8.0 (from llama-index-core)\n",
      "  Downloading typing_inspect-0.9.0-py3-none-any.whl.metadata (1.5 kB)\n",
      "Collecting wrapt (from llama-index-core)\n",
      "  Downloading wrapt-1.16.0-cp312-cp312-manylinux_2_17_aarch64.manylinux2014_aarch64.whl.metadata (6.6 kB)\n",
      "Collecting pyvis<0.4.0,>=0.3.2 (from llama-index-utils-workflow)\n",
      "  Downloading pyvis-0.3.2-py3-none-any.whl.metadata (1.7 kB)\n",
      "Requirement already satisfied: anyio<5,>=3.5.0 in /opt/conda/lib/python3.12/site-packages (from openai) (4.6.2.post1)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in /opt/conda/lib/python3.12/site-packages (from openai) (1.9.0)\n",
      "Collecting jiter<1,>=0.4.0 (from openai)\n",
      "  Downloading jiter-0.7.1-cp312-cp312-manylinux_2_17_aarch64.manylinux2014_aarch64.whl.metadata (5.2 kB)\n",
      "Requirement already satisfied: sniffio in /opt/conda/lib/python3.12/site-packages (from openai) (1.3.1)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /opt/conda/lib/python3.12/site-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core) (2.4.3)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /opt/conda/lib/python3.12/site-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core) (1.3.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /opt/conda/lib/python3.12/site-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core) (24.2.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /opt/conda/lib/python3.12/site-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core) (1.5.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /opt/conda/lib/python3.12/site-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core) (6.1.0)\n",
      "Requirement already satisfied: yarl<2.0,>=1.12.0 in /opt/conda/lib/python3.12/site-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core) (1.16.0)\n",
      "Requirement already satisfied: idna>=2.8 in /opt/conda/lib/python3.12/site-packages (from anyio<5,>=3.5.0->openai) (3.10)\n",
      "Requirement already satisfied: certifi in /opt/conda/lib/python3.12/site-packages (from httpx->llama-index-core) (2024.8.30)\n",
      "Requirement already satisfied: httpcore==1.* in /opt/conda/lib/python3.12/site-packages (from httpx->llama-index-core) (1.0.6)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in /opt/conda/lib/python3.12/site-packages (from httpcore==1.*->httpx->llama-index-core) (0.14.0)\n",
      "Requirement already satisfied: click in /opt/conda/lib/python3.12/site-packages (from nltk>3.8.1->llama-index-core) (8.1.7)\n",
      "Requirement already satisfied: joblib in /opt/conda/lib/python3.12/site-packages (from nltk>3.8.1->llama-index-core) (1.4.2)\n",
      "Collecting regex>=2021.8.3 (from nltk>3.8.1->llama-index-core)\n",
      "  Downloading regex-2024.11.6-cp312-cp312-manylinux_2_17_aarch64.manylinux2014_aarch64.whl.metadata (40 kB)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /opt/conda/lib/python3.12/site-packages (from pydantic<3.0.0,>=2.7.0->llama-index-core) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.23.4 in /opt/conda/lib/python3.12/site-packages (from pydantic<3.0.0,>=2.7.0->llama-index-core) (2.23.4)\n",
      "Requirement already satisfied: ipython>=5.3.0 in /opt/conda/lib/python3.12/site-packages (from pyvis<0.4.0,>=0.3.2->llama-index-utils-workflow) (8.29.0)\n",
      "Requirement already satisfied: jinja2>=2.9.6 in /opt/conda/lib/python3.12/site-packages (from pyvis<0.4.0,>=0.3.2->llama-index-utils-workflow) (3.1.4)\n",
      "Collecting jsonpickle>=1.4.1 (from pyvis<0.4.0,>=0.3.2->llama-index-utils-workflow)\n",
      "  Downloading jsonpickle-4.0.0-py3-none-any.whl.metadata (8.2 kB)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.12/site-packages (from requests>=2.31.0->llama-index-core) (3.4.0)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.12/site-packages (from requests>=2.31.0->llama-index-core) (2.2.3)\n",
      "Requirement already satisfied: greenlet!=0.4.17 in /opt/conda/lib/python3.12/site-packages (from SQLAlchemy>=1.4.49->SQLAlchemy[asyncio]>=1.4.49->llama-index-core) (3.1.1)\n",
      "Collecting mypy-extensions>=0.3.0 (from typing-inspect>=0.8.0->llama-index-core)\n",
      "  Downloading mypy_extensions-1.0.0-py3-none-any.whl.metadata (1.1 kB)\n",
      "Collecting marshmallow<4.0.0,>=3.18.0 (from dataclasses-json->llama-index-core)\n",
      "  Downloading marshmallow-3.23.1-py3-none-any.whl.metadata (7.5 kB)\n",
      "Requirement already satisfied: decorator in /opt/conda/lib/python3.12/site-packages (from ipython>=5.3.0->pyvis<0.4.0,>=0.3.2->llama-index-utils-workflow) (5.1.1)\n",
      "Requirement already satisfied: jedi>=0.16 in /opt/conda/lib/python3.12/site-packages (from ipython>=5.3.0->pyvis<0.4.0,>=0.3.2->llama-index-utils-workflow) (0.19.1)\n",
      "Requirement already satisfied: matplotlib-inline in /opt/conda/lib/python3.12/site-packages (from ipython>=5.3.0->pyvis<0.4.0,>=0.3.2->llama-index-utils-workflow) (0.1.7)\n",
      "Requirement already satisfied: prompt-toolkit<3.1.0,>=3.0.41 in /opt/conda/lib/python3.12/site-packages (from ipython>=5.3.0->pyvis<0.4.0,>=0.3.2->llama-index-utils-workflow) (3.0.48)\n",
      "Requirement already satisfied: pygments>=2.4.0 in /opt/conda/lib/python3.12/site-packages (from ipython>=5.3.0->pyvis<0.4.0,>=0.3.2->llama-index-utils-workflow) (2.18.0)\n",
      "Requirement already satisfied: stack-data in /opt/conda/lib/python3.12/site-packages (from ipython>=5.3.0->pyvis<0.4.0,>=0.3.2->llama-index-utils-workflow) (0.6.2)\n",
      "Requirement already satisfied: traitlets>=5.13.0 in /opt/conda/lib/python3.12/site-packages (from ipython>=5.3.0->pyvis<0.4.0,>=0.3.2->llama-index-utils-workflow) (5.14.3)\n",
      "Requirement already satisfied: pexpect>4.3 in /opt/conda/lib/python3.12/site-packages (from ipython>=5.3.0->pyvis<0.4.0,>=0.3.2->llama-index-utils-workflow) (4.9.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.12/site-packages (from jinja2>=2.9.6->pyvis<0.4.0,>=0.3.2->llama-index-utils-workflow) (3.0.2)\n",
      "Requirement already satisfied: packaging>=17.0 in /opt/conda/lib/python3.12/site-packages (from marshmallow<4.0.0,>=3.18.0->dataclasses-json->llama-index-core) (24.1)\n",
      "Requirement already satisfied: propcache>=0.2.0 in /opt/conda/lib/python3.12/site-packages (from yarl<2.0,>=1.12.0->aiohttp<4.0.0,>=3.8.6->llama-index-core) (0.2.0)\n",
      "Requirement already satisfied: parso<0.9.0,>=0.8.3 in /opt/conda/lib/python3.12/site-packages (from jedi>=0.16->ipython>=5.3.0->pyvis<0.4.0,>=0.3.2->llama-index-utils-workflow) (0.8.4)\n",
      "Requirement already satisfied: ptyprocess>=0.5 in /opt/conda/lib/python3.12/site-packages (from pexpect>4.3->ipython>=5.3.0->pyvis<0.4.0,>=0.3.2->llama-index-utils-workflow) (0.7.0)\n",
      "Requirement already satisfied: wcwidth in /opt/conda/lib/python3.12/site-packages (from prompt-toolkit<3.1.0,>=3.0.41->ipython>=5.3.0->pyvis<0.4.0,>=0.3.2->llama-index-utils-workflow) (0.2.13)\n",
      "Requirement already satisfied: executing>=1.2.0 in /opt/conda/lib/python3.12/site-packages (from stack-data->ipython>=5.3.0->pyvis<0.4.0,>=0.3.2->llama-index-utils-workflow) (2.1.0)\n",
      "Requirement already satisfied: asttokens>=2.1.0 in /opt/conda/lib/python3.12/site-packages (from stack-data->ipython>=5.3.0->pyvis<0.4.0,>=0.3.2->llama-index-utils-workflow) (2.4.1)\n",
      "Requirement already satisfied: pure-eval in /opt/conda/lib/python3.12/site-packages (from stack-data->ipython>=5.3.0->pyvis<0.4.0,>=0.3.2->llama-index-utils-workflow) (0.2.3)\n",
      "Requirement already satisfied: six>=1.12.0 in /opt/conda/lib/python3.12/site-packages (from asttokens>=2.1.0->stack-data->ipython>=5.3.0->pyvis<0.4.0,>=0.3.2->llama-index-utils-workflow) (1.16.0)\n",
      "Downloading llama_index_core-0.11.23-py3-none-any.whl (1.6 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m21.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading llama_index_utils_workflow-0.2.2-py3-none-any.whl (2.9 kB)\n",
      "Downloading asyncio-3.4.3-py3-none-any.whl (101 kB)\n",
      "Downloading llama_index_llms_openai-0.2.16-py3-none-any.whl (13 kB)\n",
      "Downloading tavily_python-0.5.0-py3-none-any.whl (14 kB)\n",
      "Downloading openai-1.54.4-py3-none-any.whl (389 kB)\n",
      "Downloading Deprecated-1.2.15-py2.py3-none-any.whl (9.9 kB)\n",
      "Downloading dirtyjson-1.0.8-py3-none-any.whl (25 kB)\n",
      "Downloading filetype-1.2.0-py2.py3-none-any.whl (19 kB)\n",
      "Downloading jiter-0.7.1-cp312-cp312-manylinux_2_17_aarch64.manylinux2014_aarch64.whl (328 kB)\n",
      "Downloading nltk-3.9.1-py3-none-any.whl (1.5 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.5/1.5 MB\u001b[0m \u001b[31m34.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading numpy-1.26.4-cp312-cp312-manylinux_2_17_aarch64.manylinux2014_aarch64.whl (13.9 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.9/13.9 MB\u001b[0m \u001b[31m51.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading pyvis-0.3.2-py3-none-any.whl (756 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m756.0/756.0 kB\u001b[0m \u001b[31m46.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading tenacity-8.5.0-py3-none-any.whl (28 kB)\n",
      "Downloading tiktoken-0.8.0-cp312-cp312-manylinux_2_17_aarch64.manylinux2014_aarch64.whl (1.1 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m46.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading typing_inspect-0.9.0-py3-none-any.whl (8.8 kB)\n",
      "Downloading wrapt-1.16.0-cp312-cp312-manylinux_2_17_aarch64.manylinux2014_aarch64.whl (86 kB)\n",
      "Downloading dataclasses_json-0.6.7-py3-none-any.whl (28 kB)\n",
      "Downloading jsonpickle-4.0.0-py3-none-any.whl (46 kB)\n",
      "Downloading marshmallow-3.23.1-py3-none-any.whl (49 kB)\n",
      "Downloading mypy_extensions-1.0.0-py3-none-any.whl (4.7 kB)\n",
      "Downloading regex-2024.11.6-cp312-cp312-manylinux_2_17_aarch64.manylinux2014_aarch64.whl (794 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m795.0/795.0 kB\u001b[0m \u001b[31m46.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: filetype, dirtyjson, asyncio, wrapt, tenacity, regex, numpy, mypy-extensions, marshmallow, jsonpickle, jiter, typing-inspect, tiktoken, nltk, deprecated, tavily-python, openai, dataclasses-json, pyvis, llama-index-core, llama-index-utils-workflow, llama-index-llms-openai\n",
      "  Attempting uninstall: numpy\n",
      "    Found existing installation: numpy 2.0.2\n",
      "    Uninstalling numpy-2.0.2:\n",
      "      Successfully uninstalled numpy-2.0.2\n",
      "Successfully installed asyncio-3.4.3 dataclasses-json-0.6.7 deprecated-1.2.15 dirtyjson-1.0.8 filetype-1.2.0 jiter-0.7.1 jsonpickle-4.0.0 llama-index-core-0.11.23 llama-index-llms-openai-0.2.16 llama-index-utils-workflow-0.2.2 marshmallow-3.23.1 mypy-extensions-1.0.0 nltk-3.9.1 numpy-1.26.4 openai-1.54.4 pyvis-0.3.2 regex-2024.11.6 tavily-python-0.5.0 tenacity-8.5.0 tiktoken-0.8.0 typing-inspect-0.9.0 wrapt-1.16.0\n"
     ]
    }
   ],
   "source": [
    "!pip install llama-index-core llama-index-utils-workflow asyncio llama-index-llms-openai tavily-python openai"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6171dc7a-4c0c-4f45-81d8-90be33e4fd9d",
   "metadata": {},
   "source": [
    "### Import Necessary Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7907cd65-ce72-4e4c-907d-2ff2384c953e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import getpass\n",
    "from llama_index.core.workflow import (\n",
    "    StartEvent,\n",
    "    StopEvent,\n",
    "    Workflow,\n",
    "    step,\n",
    "    Context,\n",
    "    Event\n",
    ")\n",
    "from llama_index.core.prompts import PromptTemplate\n",
    "from llama_index.llms.openai import OpenAI\n",
    "from tavily import TavilyClient\n",
    "from pydantic import BaseModel, Field\n",
    "from typing import Annotated\n",
    "from openai import OpenAI as dalle3 # use specifically for image generation\n",
    "from PIL import Image\n",
    "from io import BytesIO\n",
    "import requests\n",
    "import uuid\n",
    "import os\n",
    "import asyncio"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72434945-8409-4bde-b413-5465b50a1727",
   "metadata": {},
   "source": [
    "### Set Environment Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e89296d2-4533-46e4-8bd1-afe8598b36a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      " ········\n",
      " ········\n"
     ]
    }
   ],
   "source": [
    "tavily_api_key = getpass.getpass()\n",
    "openai_api_key = getpass.getpass()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54fb4c8b-c3ec-487c-8c52-4fd6b18cf434",
   "metadata": {},
   "source": [
    "## Setting up Utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4adb3b53-8cee-4dab-86e3-86f22ccdaf5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TavilySearchInput(BaseModel):\n",
    "    query: Annotated[str, Field(description=\"The search query string\")]\n",
    "    max_results: Annotated[\n",
    "        int, Field(description=\"Maximum number of results to return\", ge=1, le=10)\n",
    "    ] = 5\n",
    "    search_depth: Annotated[\n",
    "        str,\n",
    "        Field(\n",
    "            description=\"Search depth: 'basic' or 'advanced'\",\n",
    "            choices=[\"basic\", \"advanced\"],\n",
    "        ),\n",
    "    ] = \"basic\"\n",
    "\n",
    "def tavily_search(query: Annotated[TavilySearchInput, \"Input for Tavily search\"]):\n",
    "    tavily_client = TavilyClient(api_key=tavily_api_key)\n",
    "    # Perform the search\n",
    "    response = tavily_client.search(\n",
    "        query=query.query,\n",
    "        max_results=query.max_results,\n",
    "        search_depth=query.search_depth,\n",
    "    )\n",
    "\n",
    "    # Format the results\n",
    "    formatted_results = []\n",
    "    for result in response.get(\"results\", []):\n",
    "        formatted_results.append(\n",
    "            f\"Title: {result['title']}\\\\nURL: {result['url']}\\\\nContent: {result['content']}\\\\n\"\n",
    "        )\n",
    "\n",
    "    return \"\\\\n\".join(formatted_results)\n",
    "\n",
    "def save_file(content, uuid, type=\"Blog\"):\n",
    "        # Define the filename for the markdown file\n",
    "        directory = uuid\n",
    "        file_name = f\"{type}_{uuid}.md\"\n",
    "        file_path = os.path.join(directory, file_name)\n",
    "    \n",
    "        try:\n",
    "            # Create the directory if it doesn't exist\n",
    "            os.makedirs(directory, exist_ok=True)\n",
    "    \n",
    "            # Open the file in write mode ('w') and save the blog content\n",
    "            with open(file_path, 'w') as file:\n",
    "                file.write(f\"# {content}\")\n",
    "            print(f\"{type} post saved to {file_path}.\")\n",
    "        except Exception as e:\n",
    "            print(\"An error occurred while saving the blog post:\", e)\n",
    "\n",
    "async def generate_image_with_retries(client, prompt, max_retries=3, delay=2):\n",
    "    attempt = 0\n",
    "    while attempt < max_retries:\n",
    "        try:\n",
    "            # Attempt to generate the image using the dalle3 API\n",
    "            response = client.images.generate(\n",
    "                model=\"dall-e-3\",\n",
    "                prompt=prompt,\n",
    "                size=\"1024x1024\",\n",
    "                quality=\"standard\",\n",
    "                n=1,\n",
    "            )\n",
    "            return response\n",
    "        except Exception as e:\n",
    "            attempt += 1\n",
    "            print(f\"Attempt {attempt} failed with error: {e}\")\n",
    "            if attempt < max_retries:\n",
    "                await asyncio.sleep(delay)\n",
    "                print(f\"Retrying after {delay} seconds...\")\n",
    "            else:\n",
    "                print(\"Exceeded maximum retries.\")\n",
    "                raise"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45dd46f0-f977-40e8-ba24-ab54bc98e323",
   "metadata": {},
   "source": [
    "## Setup Prompt Instructions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0f11ba70-2e45-453f-8af9-79ea344938db",
   "metadata": {},
   "outputs": [],
   "source": [
    "from prompts.prompts import *\n",
    "\n",
    "blog_template = BLOG_TEMPLATE\n",
    "blog_and_research_template = BLOG_AND_RESEARCH_TEMPLATE\n",
    "image_prompt_instructions = IMAGE_GENERATION_TEMPLATE\n",
    "linked_in_template = LINKED_IN_TEMPLATE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56995989-5f64-4e50-b7d7-300101dcc3fd",
   "metadata": {},
   "source": [
    "## Setting up Workflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "791d7041-d735-4466-af5d-c17cb80ebb89",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResearchEvent(Event):\n",
    "    query: str\n",
    "    uuid: str\n",
    "\n",
    "class BlogEvent(Event):\n",
    "    query: str\n",
    "    research: str\n",
    "    uuid: str\n",
    "\n",
    "class BlogWithoutResearch(Event):\n",
    "    query: str\n",
    "    uuid: str\n",
    "\n",
    "class SocialMediaEvent(Event):\n",
    "    blog: str\n",
    "    uuid: str\n",
    "\n",
    "class SocialMediaCompleteEvent(Event):\n",
    "    result: str\n",
    "\n",
    "class IllustratorEvent(Event):\n",
    "    blog: str\n",
    "    \n",
    "class IllustratorCompleteEvent(Event):\n",
    "    result: str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "86aa5b3f-1825-4405-bfe4-3281f6d2d987",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ContentCreationWorkflow(Workflow):\n",
    "\n",
    "    @step\n",
    "    async def start(self, ctx: Context, ev: StartEvent) -> ResearchEvent | BlogWithoutResearch :\n",
    "        print(\"Starting content creation\", ev.query)\n",
    "        id = str(uuid.uuid4())\n",
    "        if (ev.research) is False:\n",
    "            return BlogWithoutResearch(query=ev.query, uuid=id)\n",
    "        return ResearchEvent(query=ev.query, uuid=id)\n",
    "\n",
    "    @step\n",
    "    async def step_research(self, ctx: Context, ev: ResearchEvent) -> BlogEvent:\n",
    "        print(\"Researching users query\")\n",
    "        search_input = TavilySearchInput(\n",
    "            query=ev.query,\n",
    "            max_results=3,\n",
    "            search_depth=\"basic\")\n",
    "        research = tavily_search(search_input)\n",
    "        return BlogEvent(query=ev.query, research=research, uuid=ev.uuid)\n",
    "\n",
    "    @step\n",
    "    async def step_blog_without_research(self, ctx: Context, ev: BlogWithoutResearch) -> SocialMediaEvent | IllustratorEvent:\n",
    "        print(\"Writing blog post without research\")\n",
    "        print(\"uuid\", ev.uuid)\n",
    "        llm = OpenAI(model=\"gpt-4o-mini\", api_key=openai_api_key)\n",
    "        prompt = blog_template.format(query_str=ev.query)\n",
    "        result = await llm.acomplete(prompt, formatted=True)\n",
    "        save_file(result.text, ev.uuid)\n",
    "        print(result)\n",
    "        ctx.send_event(SocialMediaEvent(blog=result.text, uuid=ev.uuid))\n",
    "        ctx.send_event(IllustratorEvent(blog=result.text, uuid=ev.uuid))\n",
    "                        \n",
    "    @step\n",
    "    async def step_blog(self, ctx: Context, ev: BlogEvent) -> SocialMediaEvent | IllustratorEvent:\n",
    "        print(\"Writing blog post\")\n",
    "\n",
    "        llm = OpenAI(model=\"gpt-4o-mini\", api_key=openai_api_key)\n",
    "        prompt = blog_and_research_template.format(query_str=ev.query, research=ev.research)\n",
    "        result = await llm.acomplete(prompt, formatted=True)\n",
    "\n",
    "        save_file(result.text, ev.uuid)\n",
    "        ctx.send_event(SocialMediaEvent(blog=result.text, uuid=ev.uuid))\n",
    "        ctx.send_event(IllustratorEvent(blog=result.text, uuid=ev.uuid))\n",
    "\n",
    "    @step\n",
    "    async def step_social_media(self, ctx: Context, ev: SocialMediaEvent) -> SocialMediaCompleteEvent:\n",
    "        print(\"Writing social media post\")\n",
    "        llm = OpenAI(model=\"gpt-4o-mini\", api_key=openai_api_key)\n",
    "        prompt = linked_in_template.format(blog_content=ev.blog)\n",
    "        results = await llm.acomplete(prompt, formatted=True)\n",
    "        save_file(results.text, ev.uuid, type=\"LinkedIn\")\n",
    "        return SocialMediaCompleteEvent(result=\"LinkedIn post written\")\n",
    "\n",
    "    @step\n",
    "    async def step_illustrator(self, ctx: Context, ev:IllustratorEvent) -> IllustratorCompleteEvent:\n",
    "        print(\"Generating image\")\n",
    "        llm = OpenAI(model=\"gpt-4o-mini\", api_key=openai_api_key)\n",
    "        image_prompt_instruction_generator = image_prompt_instructions.format(blog_post=ev.blog)\n",
    "        image_prompt = await llm.acomplete(image_prompt_instruction_generator, formatted=True)\n",
    "        \n",
    "        client = dalle3(api_key=openai_api_key)\n",
    "        response = await generate_image_with_retries(client, image_prompt.text)\n",
    "        image_url = response.data[0].url\n",
    "        response = requests.get(image_url)\n",
    "        image = Image.open(BytesIO(response.content))\n",
    "        \n",
    "        directory = f'./{ev.uuid}'\n",
    "        os.makedirs(directory, exist_ok=True)\n",
    "        image.save(f'{directory}/generated_image.png')\n",
    "        image.save(f'{ev.uuid}/generated_image.png')\n",
    "        \n",
    "        return IllustratorCompleteEvent(result=\"Images drawn\")\n",
    "\n",
    "    @step\n",
    "    async def step_collection(self, ctx: Context, ev: SocialMediaCompleteEvent | IllustratorCompleteEvent) -> StopEvent:\n",
    "        if (\n",
    "            ctx.collect_events(\n",
    "                ev,\n",
    "                [SocialMediaCompleteEvent, IllustratorCompleteEvent]\n",
    "            ) is None\n",
    "        ) : return None\n",
    "        return StopEvent(result=\"Done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7167ef3b-6b4e-4b59-9092-25d453724992",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting content creation Albert Einstein\n",
      "Researching users query\n",
      "Writing blog post\n",
      "Blog post saved to 093e7f9e-d409-4e70-adc8-75397962ea05/Blog_093e7f9e-d409-4e70-adc8-75397962ea05.md.\n",
      "Writing social media post\n",
      "LinkedIn post saved to 093e7f9e-d409-4e70-adc8-75397962ea05/LinkedIn_093e7f9e-d409-4e70-adc8-75397962ea05.md.\n",
      "Done\n"
     ]
    }
   ],
   "source": [
    "w = ContentCreationWorkflow(timeout=120, verbose=False)\n",
    "result = await w.run(query=\"Albert Einstein\", research=True)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58465141-4164-42da-8d13-2fe641edb691",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
